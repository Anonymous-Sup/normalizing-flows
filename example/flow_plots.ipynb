{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow training and visualization\n",
    "In this notebook, we train normalizing flows to fit predefined prior distributions, testing their expressivity. The plots are generated to visualize the learned distributions for given layers $K$, and the training loss is plotted to compare the expressivity of different flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import required packages\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import normflow as nf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PyTorch version: %s\" % torch.__version__)\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device: %s\" % dev)\n",
    "\n",
    "#z shape is (batch_size, num_samples, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior target distribution visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = []\n",
    "priors.append(nf.distributions.TwoModes(2.0, 0.2))\n",
    "priors.append(nf.distributions.Sinusoidal(0.4, 4))\n",
    "priors.append(nf.distributions.Sinusoidal_gap(0.4, 4))\n",
    "priors.append(nf.distributions.Sinusoidal_split(0.4, 4))\n",
    "priors.append(nf.distributions.Smiley(0.15))\n",
    "\n",
    "\n",
    "# Plot prior distributions\n",
    "grid_size = 200\n",
    "grid_length = 4.0\n",
    "grid_shape = ([-grid_length, grid_length], [-grid_length, grid_length])\n",
    "\n",
    "space_mesh = torch.linspace(-grid_length, grid_length, grid_size)\n",
    "xx, yy = torch.meshgrid(space_mesh, space_mesh)\n",
    "z = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2)\n",
    "\n",
    "K_arr = [2, 8, 32]\n",
    "max_iter = 30000\n",
    "batch_size = 512\n",
    "num_samples = 256\n",
    "save_iter = 1000\n",
    "\n",
    "for k in range(5):\n",
    "    log_prob = priors[k].log_prob(z)\n",
    "    prob = torch.exp(log_prob)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.pcolormesh(xx, yy, prob)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_types = (\"Planar\", \"Radial\", \"NICE\", \"RealNVP\", \"Glow\")\n",
    "max_iter = 100\n",
    "batch_size = 128\n",
    "num_samples = 128\n",
    "save_iter = 50\n",
    "\n",
    "for name in flow_types:\n",
    "    K_arr = [2, 8, 32]\n",
    "    for K in K_arr:\n",
    "        print(\"Flow type {} with K = {}\".format(name, K))\n",
    "        for k in range(0,4):\n",
    "            if k == 0 or k == 4:\n",
    "                anneal_iter = 10000\n",
    "            else: # turn annealing off when fitting to sinusoidal distributions\n",
    "                anneal_iter = 1\n",
    "        \n",
    "            flows = []\n",
    "            b = torch.tensor([0,1])\n",
    "            for i in range(K):\n",
    "                if name == \"Planar\":\n",
    "                    flows += [nf.flows.Planar((2,))]\n",
    "                elif name == \"Radial\":\n",
    "                    flows += [nf.flows.Radial((2,))]\n",
    "                elif name == \"NICE\":\n",
    "                    flows += [nf.flows.MaskedAffineFlow(b, nf.nets.MLP([2, 8, 2]), \\\n",
    "                                                  nf.nets.MLP([2, 8, 2]), scale=False)]\n",
    "                elif name == \"RealNVP\":\n",
    "                    flows += [nf.flows.MaskedAffineFlow(b, nf.nets.MLP([2, 8, 2]), \\\n",
    "                                                  nf.nets.MLP([2, 8, 2]))]\n",
    "                elif name == \"Glow\":\n",
    "                    flows += [nf.flows.Glow(b, nf.nets.MLP([2, 8, 2]), \\\n",
    "                                                  nf.nets.MLP([2, 8, 2]))]\n",
    "                b = 1-b # parity alternation for mask\n",
    "\n",
    "            q0 = nf.distributions.ConstDiagGaussian(np.zeros(2), 2.0)\n",
    "            nfm = nf.NormalizingFlow(prior=priors[k], q0=q0, flows=flows)\n",
    "            nfm.to(dev) # Move model on GPU if available\n",
    "    \n",
    "            # Train model\n",
    "            loss_hist = np.array([])\n",
    "            log_q_hist = np.array([])\n",
    "            log_p_hist = np.array([])\n",
    "            x = torch.zeros(batch_size, device=dev)\n",
    "\n",
    "            optimizer = torch.optim.Adam(nfm.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "            for it in tqdm(range(max_iter)):\n",
    "                optimizer.zero_grad()\n",
    "                _, log_q, log_p = nfm(x, num_samples)\n",
    "                mean_log_q = torch.mean(log_q)\n",
    "                mean_log_p = torch.mean(log_p)\n",
    "                loss = mean_log_q - np.min([1.0, 0.01 + it / anneal_iter]) * mean_log_p\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                if (it + 1) % save_iter == 0:\n",
    "                    loss_hist = np.append(loss_hist, loss.cpu().data.numpy())\n",
    "                    log_q_hist = np.append(log_q_hist, mean_log_q.cpu().data.numpy())\n",
    "                    log_p_hist = np.append(log_p_hist, mean_log_p.cpu().data.numpy())\n",
    "\n",
    "            # Plot learned posterior distribution\n",
    "            z, _, _ = nfm(x, num_samples=num_samples)\n",
    "            z_np = z.cpu().data.numpy()\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.hist2d(z_np[:, :, 0].flatten(), z_np[:, :, 1].flatten(), (grid_size, grid_size), grid_shape)\n",
    "            plt.show()\n",
    "            np.save(\"{}-K={}-k={}\".format(name,K,k), (z_np, loss.cpu().data.numpy()))\n",
    "    \n",
    "            # Plot training history\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.plot(loss_hist, label='loss')\n",
    "            plt.plot(log_q_hist, label='log_q')\n",
    "            plt.plot(log_p_hist, label='log_p')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressivity plot of flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "K_arr = [2, 8, 32]\n",
    "nrows=5\n",
    "ncols=7\n",
    "axes = [ fig.add_subplot(nrows, ncols, r * ncols + c + 1) for r in range(0, nrows) for c in range(0, ncols) ]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "grid_size = 100\n",
    "grid_length = 4.0\n",
    "grid_shape = ([-grid_length, grid_length], [-grid_length, grid_length])\n",
    "\n",
    "space_mesh = torch.linspace(-grid_length, grid_length, grid_size)\n",
    "xx, yy = torch.meshgrid(space_mesh, space_mesh)\n",
    "z = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2)\n",
    "axes[0].annotate('Target', xy=(0.5, 1.10), xytext=(0.5, 1.20), xycoords='axes fraction', \n",
    "            fontsize=24, ha='center', va='bottom',\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=1.5, lengthB=0.2', lw=2.0))\n",
    "for k in range(5):\n",
    "    axes[k*ncols].set_ylabel('{}'.format(k+1), rotation=0, fontsize=20, labelpad=15)\n",
    "    log_prob = priors[k].log_prob(z)\n",
    "    prob = torch.exp(log_prob)\n",
    "    axes[k*ncols + 0].pcolormesh(xx, yy, prob)\n",
    "\n",
    "\n",
    "for l in range(len(K_arr)):\n",
    "    K = K_arr[l]\n",
    "    if l == 1:\n",
    "        axes[0*ncols + l+1].annotate('Planar flows', xy=(0.5, 1.10), xytext=(0.5, 1.20), xycoords='axes fraction', \n",
    "            fontsize=24, ha='center', va='bottom',\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=6.0, lengthB=0.2', lw=2.0))\n",
    "    axes[4*ncols + l+1].set_xlabel('K = {}'.format(K), fontsize=20)\n",
    "    for k in range(5):\n",
    "        z_np, _ = np.load(\"K={}-k={}.npy\".format(K,k), allow_pickle=True)\n",
    "        axes[k*ncols + l+1].hist2d(z_np[:, :, 0].flatten(), z_np[:, :, 1].flatten(), (grid_size, grid_size), grid_shape)\n",
    "        \n",
    "for l in range(len(K_arr)):\n",
    "    K = K_arr[l]\n",
    "    if l == 1:\n",
    "        axes[0*ncols + l+1+len(K_arr)].annotate('Radial flows', xy=(0.5, 1.10), xytext=(0.5, 1.20), xycoords='axes fraction', \n",
    "            fontsize=24, ha='center', va='bottom',\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=6.0, lengthB=0.2', lw=2.0))\n",
    "    axes[4*ncols + l+1+len(K_arr)].set_xlabel('K = {}'.format(K), fontsize=20)\n",
    "    for k in range(5):\n",
    "        z_np, _ = np.load(\"rad_K={}-k={}.npy\".format(K,k), allow_pickle=True)\n",
    "        axes[k*ncols + l+1+len(K_arr)].hist2d(z_np[:, :, 0].flatten(), z_np[:, :, 1].flatten(), (grid_size, grid_size), grid_shape)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.02, wspace=0.02)\n",
    "\n",
    "for l in range(1,4):\n",
    "    for k in range(5):\n",
    "        pos1 = axes[k*ncols + l].get_position() # get the original position \n",
    "        pos2 = [pos1.x0 + 0.01, pos1.y0,  pos1.width, pos1.height] \n",
    "        axes[k*ncols + l].set_position(pos2) # set a new position\n",
    "        \n",
    "for l in range(4,7):\n",
    "    for k in range(5):\n",
    "        pos1 = axes[k*ncols + l].get_position() # get the original position \n",
    "        pos2 = [pos1.x0 + 0.02, pos1.y0,  pos1.width, pos1.height] \n",
    "        axes[k*ncols + l].set_position(pos2) # set a new position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Planar, Radial, Affine and Glow on given prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "k_arr = [0, 2, 4]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "markers = ['s', 'o', 'v', 'P', 'd']\n",
    "\n",
    "for k in range(len(k_arr)):\n",
    "    loss = [[] for i in repeat(None, len(flow_types))]\n",
    "    for intt, name in enumerate(flow_types):\n",
    "        for K in K_arr:\n",
    "            _, loss_v = np.load(\"{}-K={}-k={}.npy\".format(name,K,k), allow_pickle=True)\n",
    "            loss[intt].append(loss_v)\n",
    "        axes[k].plot(K_arr, loss[intt], marker=markers[intt], label=name)\n",
    "    axes[k].set_title('Target {}'.format(k_arr[k]+1), fontsize=16)\n",
    "    axes[k].set_xlabel('Flow length', fontsize=12)\n",
    "    axes[k].set_ylabel('Variational bound (nats)', fontsize=12)\n",
    "    axes[k].legend()\n",
    "    axes[k].grid('major')\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
